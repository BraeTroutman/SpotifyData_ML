{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron and Adaline ML Models Applied to the \"superheroes-NLP-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and looking at our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"text-decoration:line-through\">I chose this set because I really like superheroes, I grew up reading all of my dad's old comics that we kept in a box under my bed. This dataset is also targeted specifically towards natural language processing, a topic which I'm really interested in learning more about!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually scratch all of that I bit off more than I could chew trying to use NLP with machine learning without experience with mapping text data to numeric features that could be used for ML.\n",
    "\n",
    "Music has always been a passion in my life, and I found an interesting dataset that quantifies qualities of music from spotify and also classifies each song in terms of whether the dataset's creator likes or dislikes the song. I think it would be really interesting to do this myself with songs I like dislike.\n",
    "\n",
    "After working with the data targeting whether or not the creator of the dataset liked the song, I realized that this would likely not yield a good prediction since music taste can be eclectic and I'm also not sure how the creator went about quantifying their like or dislike. So, instead I'm using the other features in the set to predict whether the song is in mode 0 or mode 1: presumably Ionian and Dorian. These modes often have associations with different \"vibes\" to a song in the same way that the key or time signature might, so I think that we should see somewhat of a correlation between some of these features like \"danceability\" and \"liveness\" and the mode of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>target</th>\n",
       "      <th>song_title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>1082</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.683</td>\n",
       "      <td>219507</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-5.535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>179.910</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0</td>\n",
       "      <td>El Amante</td>\n",
       "      <td>Nicky Jam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>1364</td>\n",
       "      <td>0.71200</td>\n",
       "      <td>0.483</td>\n",
       "      <td>229261</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>-3.920</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>91.878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Standing Egg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>1129</td>\n",
       "      <td>0.00898</td>\n",
       "      <td>0.499</td>\n",
       "      <td>229093</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>-4.741</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>161.977</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0</td>\n",
       "      <td>Sugar, We're Goin Down</td>\n",
       "      <td>Fall Out Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.00631</td>\n",
       "      <td>0.715</td>\n",
       "      <td>213093</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>-5.379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>95.487</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.607</td>\n",
       "      <td>1</td>\n",
       "      <td>Return Of The Mack - C &amp; J Street Mix</td>\n",
       "      <td>Mark Morrison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>414</td>\n",
       "      <td>0.19500</td>\n",
       "      <td>0.605</td>\n",
       "      <td>209573</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>-5.643</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>195.978</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.136</td>\n",
       "      <td>1</td>\n",
       "      <td>Obedear</td>\n",
       "      <td>Purity Ring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  acousticness  danceability  duration_ms  energy  \\\n",
       "1082        1082       0.24300         0.683       219507   0.691   \n",
       "1364        1364       0.71200         0.483       229261   0.524   \n",
       "1129        1129       0.00898         0.499       229093   0.824   \n",
       "45            45       0.00631         0.715       213093   0.833   \n",
       "414          414       0.19500         0.605       209573   0.732   \n",
       "\n",
       "      instrumentalness  key  liveness  loudness  mode  speechiness    tempo  \\\n",
       "1082            0.0000    8    0.1400    -5.535     1       0.0432  179.910   \n",
       "1364            0.0000   11    0.0975    -3.920     1       0.0261   91.878   \n",
       "1129            0.0000    7    0.1630    -4.741     1       0.0794  161.977   \n",
       "45              0.0000    2    0.1640    -5.379     1       0.1080   95.487   \n",
       "414             0.0135    6    0.2460    -5.643     0       0.1500  195.978   \n",
       "\n",
       "      time_signature  valence  target                             song_title  \\\n",
       "1082             4.0    0.746       0                              El Amante   \n",
       "1364             4.0    0.519       0                                 Wonder   \n",
       "1129             4.0    0.692       0                 Sugar, We're Goin Down   \n",
       "45               4.0    0.607       1  Return Of The Mack - C & J Street Mix   \n",
       "414              4.0    0.136       1                                Obedear   \n",
       "\n",
       "             artist  \n",
       "1082      Nicky Jam  \n",
       "1364   Standing Egg  \n",
       "1129   Fall Out Boy  \n",
       "45    Mark Morrison  \n",
       "414     Purity Ring  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_data = pd.read_csv('spotify.csv')\n",
    "music_data = music_data.sample(frac=1)\n",
    "music_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron(object):\n",
    "    \"\"\"perceptron linear classifier\"\"\"\n",
    "    \n",
    "    # I added the keyword parameter threshold to allow the user to specify the threshold\n",
    "    def __init__(self, learning_rate=0.1, epochs=50, random_seed=1, threshold=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.random_seed = random_seed\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit (self, X, y):\n",
    "        \"\"\"Initialize and iteratively update weights\"\"\"\n",
    "        unnormalized_weights = np.random.RandomState(self.random_seed)\n",
    "        self.weights = unnormalized_weights.normal(loc=0.0, scale=0.1, size= 1 + X.shape[1]) # initialize weights to small random numbers\n",
    "    \n",
    "        self.errors_ = [] # Will keep track of the number of missclassifications\n",
    "    \n",
    "        for _ in range(self.epochs): #iterate over the data set epochs times\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                delta_weights = self.learning_rate * (target - self.predict(xi)) # compare the predicted value to the desired, and determine change in weights\n",
    "                self.weights[1:] += delta_weights * xi # all weights by delta_weights\n",
    "                self.weights[0] += delta_weights # update bias unit\n",
    "                errors += int(delta_weights != 0.0) # add 1 to the number of errors if the weight changed: otherwise add 0\n",
    "            self.errors_.append(errors) # append number of errors to errors list so we can plot convergence later\n",
    "        return self\n",
    "    \n",
    "    def net_input (self, X):\n",
    "        \"\"\"calculate net input\"\"\"\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "    \n",
    "    def predict (self, X):\n",
    "        return np.where(self.net_input(X) >= self.threshold, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fns for running Adaline and analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_and_misclasses(prediction, labels):\n",
    "    \"\"\"Fn to determine accuracy\"\"\"\n",
    "    missclassifications = 0\n",
    "    correct_predictions = len(labels)\n",
    "    for a,b in zip(prediction, labels):\n",
    "        if a != b:\n",
    "            missclassifications += 1\n",
    "            correct_predictions -= 1\n",
    "    return (correct_predictions / len(labels), missclassifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fit_test(c1, c2, c3, testtrain_ratio, dataframe=music_data, verbose=False, learning_rate=0.1, epochs=50, threshold=0):\n",
    "    \"\"\"split data from feature columns c1 and c2 into train and test sets at tt_ratio proportions and fit/test a perceptron\"\"\"\n",
    "    \n",
    "    # get the integer indeces corresponding to the column names passed to split_fit_test\n",
    "    c1_idx = dataframe.columns.get_loc(c1)\n",
    "    c2_idx = dataframe.columns.get_loc(c2)\n",
    "    c3_idx = dataframe.columns.get_loc(c2)\n",
    "    \n",
    "    # number of rows of dataframe which will belong to the training set (we know the number in the test set from this implicitly)\n",
    "    num_train = len(dataframe.index) - int(len(dataframe.index) * testtrain_ratio)\n",
    "    \n",
    "    # Training set\n",
    "    y_train = dataframe.iloc[:num_train,9].values # the array of target values: 2 for benign, 4 for malignant\n",
    "    y_train = np.where(y_train == 1, 1, -1) # change class labels 2 and 4 to -1 and 1 respectively\n",
    "    X_train = dataframe.iloc[:num_train, [c1_idx,c2_idx,c3_idx]].values\n",
    "    \n",
    "    # feature scaling to standardize the distribution of values in our training set\n",
    "    X_train_std = np.copy(X_train)\n",
    "    X_train_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
    "    X_train_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
    "    X_train_std[:, 2] = (X_train[:, 2] - X_train[:, 2].mean()) / X_train[:, 2].std()\n",
    "    \n",
    "    # Testing set\n",
    "    y_test = dataframe.iloc[num_train:,9].values # analagous to above\n",
    "    y_test = np.where(y_test == 1, 1, -1)\n",
    "    X_test = dataframe.iloc[num_train:, [c1_idx, c2_idx, c3_idx]].values\n",
    "    \n",
    "    # Feature scaling for test set\n",
    "    X_test_std = np.copy(X_train)\n",
    "    X_test_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
    "    X_test_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
    "    X_test_std[:, 2] = (X_train[:, 2] - X_train[:, 2].mean()) / X_train[:, 2].std()\n",
    "    \n",
    "    # instantiate and train a perceptron object\n",
    "    tron = perceptron(learning_rate=learning_rate, epochs=epochs, threshold=threshold)\n",
    "    tron.fit(X_train_std, y_train)\n",
    "\n",
    "    # predict the classes of the test set and calculate accuracy\n",
    "    prediction = tron.predict(X_test_std)\n",
    "    accuracy,misclasses = accuracy_and_misclasses(prediction, y_test)\n",
    "    if verbose:\n",
    "        print(\"For features\", c1, \",\", c2, \"and\", c3, \", and test/train ratio\", testtrain_ratio, \"the perceptron had\", misclasses, \"missclassifications and had an accuracy of\", accuracy, \"\\n\")\n",
    "        \n",
    "    return (accuracy, misclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran a brute-force loop to check every possible set of features for their accuracy (it ran for about 10 minutes)\n",
    "The results were this:\n",
    "       The highest accuracy was 0.5553719008264463 for the feature set danceability , mode and key with 269 \n",
    "       missclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run:::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For features danceability , liveness and energy , and test/train ratio 0.3 the perceptron had 261 missclassifications and had an accuracy of 0.5685950413223141 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5685950413223141, 261)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_fit_test('danceability', 'liveness', 'energy', 0.3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 1: Maximize accuracy with respect to test/training set ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.6190476190476191 for test/train proportion 0.25 with 192 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_prop = 0\n",
    " \n",
    "for prop in [0.25, 0.3, 0.35, 0.40, 0.45]: # Try a few different test/train proportions\n",
    "    acc,miss = split_fit_test('danceability', 'liveness', 'energy', prop)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_prop = prop\n",
    "        \n",
    "        \n",
    "print(\"The highest accuracy was\", best_accuracy, \"for test/train proportion\", best_prop, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 2: Maximize accuracy by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.623015873015873 for learning rate 0.0001 with 190 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_rate = 0\n",
    "\n",
    "for rate in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]: # Try a few different learning rates\n",
    "    acc,miss = split_fit_test('danceability', 'liveness', 'energy', 0.25, learning_rate=rate)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_rate = rate\n",
    "\n",
    "print(\"The highest accuracy was\", best_accuracy, \"for learning rate\", best_rate, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 3: Maximize accuracy with respect to number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.626984126984127 for 75 epochs with 188 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_num_epochs = 0\n",
    "\n",
    "for n in [10, 20, 30, 40, 50, 75, 100, 200]: # Try a few different numbers of epochs\n",
    "    acc,miss = split_fit_test('danceability', 'liveness', 'energy', 0.25, learning_rate=0.3, epochs=n)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_num_epochs = n\n",
    "\n",
    "print(\"The highest accuracy was\", best_accuracy, \"for\", best_num_epochs, \"epochs with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 4: Maximize accuracy with respect to threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.6349206349206349 for the threshold 2 with 184 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_threshold = 0\n",
    "for theta in [0, 0.1, 0.01, 0.2, 0.5, 1, 2, -1, -2, 3, 4, 6]: # Try a few different tolerance thresholds\n",
    "    acc,miss = split_fit_test('danceability', 'liveness', 'energy', 0.25, learning_rate=0.3, epochs=50, threshold=theta)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_threshold = theta\n",
    "        \n",
    "print(\"The highest accuracy was\", best_accuracy, \"for the threshold\", best_threshold, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy we obtained was ~60% (it varies because of the initial shuffling of the music_data dataframe)\n",
    "This model was not as accurate as our models as applied to the cancer data set, but some of the features in this data set are based on subjective concepts such as danceability. Beyond that, there is may also simply not be a significant correlation between the features selected and the mode of the song at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
