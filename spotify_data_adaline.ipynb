{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron and Adaline ML Models Applied to the \"superheroes-NLP-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and looking at our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"text-decoration:line-through\">I chose this set because I really like superheroes, I grew up reading all of my dad's old comics that we kept in a box under my bed. This dataset is also targeted specifically towards natural language processing, a topic which I'm really interested in learning more about!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually scratch all of that I bit off more than I could chew trying to use NLP with machine learning without experience with mapping text data to numeric features that could be used for ML.\n",
    "\n",
    "Music has always been a passion in my life, and I found an interesting dataset that quantifies qualities of music from spotify and also classifies each song in terms of whether the dataset's creator likes or dislikes the song. I think it would be really interesting to do this myself with songs I like dislike.\n",
    "\n",
    "After working with the data targeting whether or not the creator of the dataset liked the song, I realized that this would likely not yield a good prediction since music taste can be eclectic and I'm also not sure how the creator went about quantifying their like or dislike. So, instead I'm using the other features in the set to predict whether the song is in mode 0 or mode 1: presumably Ionian and Dorian. These modes often have associations with different \"vibes\" to a song in the same way that the key or time signature might, so I think that we should see somewhat of a correlation between some of these features like \"danceability\" and \"liveness\" and the mode of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>target</th>\n",
       "      <th>song_title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>1418</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.523</td>\n",
       "      <td>291280</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.612</td>\n",
       "      <td>-3.755</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>117.264</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple of My Eye</td>\n",
       "      <td>Rick Ross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>881</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.479</td>\n",
       "      <td>230560</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>11</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-6.101</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>136.067</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1</td>\n",
       "      <td>Pedestrian at Best</td>\n",
       "      <td>Courtney Barnett</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1402</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.652</td>\n",
       "      <td>177090</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>5</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>104.980</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0</td>\n",
       "      <td>Run</td>\n",
       "      <td>Okdal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>639</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.797</td>\n",
       "      <td>253187</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>9</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-8.154</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>118.032</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1</td>\n",
       "      <td>Slippin’</td>\n",
       "      <td>Quadron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.532</td>\n",
       "      <td>213622</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-9.912</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>119.296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.486</td>\n",
       "      <td>1</td>\n",
       "      <td>I Know There's Gonna Be (Good Times)</td>\n",
       "      <td>Jamie xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  acousticness  danceability  duration_ms  energy  \\\n",
       "1418        1418      0.214000         0.523       291280   0.783   \n",
       "881          881      0.000005         0.479       230560   0.854   \n",
       "1402        1402      0.578000         0.652       177090   0.512   \n",
       "639          639      0.284000         0.797       253187   0.508   \n",
       "55            55      0.120000         0.532       213622   0.596   \n",
       "\n",
       "      instrumentalness  key  liveness  loudness  mode  speechiness    tempo  \\\n",
       "1418          0.000000    6     0.612    -3.755     0       0.1850  117.264   \n",
       "881           0.000057   11     0.372    -6.101     1       0.0405  136.067   \n",
       "1402          0.000007    5     0.101    -7.062     1       0.0276  104.980   \n",
       "639           0.008590    9     0.356    -8.154     0       0.0455  118.032   \n",
       "55            0.000000   11     0.504    -9.912     0       0.1870  119.296   \n",
       "\n",
       "      time_signature  valence  target                            song_title  \\\n",
       "1418             5.0    0.312       0                       Apple of My Eye   \n",
       "881              4.0    0.550       1                    Pedestrian at Best   \n",
       "1402             4.0    0.284       0                                   Run   \n",
       "639              4.0    0.958       1                              Slippin’   \n",
       "55               5.0    0.486       1  I Know There's Gonna Be (Good Times)   \n",
       "\n",
       "                artist  \n",
       "1418         Rick Ross  \n",
       "881   Courtney Barnett  \n",
       "1402             Okdal  \n",
       "639            Quadron  \n",
       "55            Jamie xx  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_data = pd.read_csv('spotify.csv')\n",
    "music_data = music_data.sample(frac=1)\n",
    "music_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\"\"\"\n",
    "    \n",
    "    # I added the keyword parameter threshold to allow the user to specify the threshold\n",
    "    def __init__(self, learning_rate=0.01, epochs=10, shuffle=True, random_seed=None, threshold=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_seed = random_seed\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Initialize and iteratively update weights\"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
    "        if not self.weights_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "        return self\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"Shuffle training data\"\"\"\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
    "        self.rgen = np.random.RandomState(self.random_seed)\n",
    "        self.weights = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
    "        self.weights_initialized = True\n",
    "        \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = (target - output)\n",
    "        self.weights[1:] += self.learning_rate * xi.dot(error)\n",
    "        self.weights[0] += self.learning_rate * error\n",
    "        cost = 0.5 * error**2\n",
    "        return cost\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= self.threshold, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fns for running Adaline and analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_and_misclasses(prediction, labels):\n",
    "    \"\"\"Fn to determine accuracy\"\"\"\n",
    "    missclassifications = 0\n",
    "    correct_predictions = len(labels)\n",
    "    for a,b in zip(prediction, labels):\n",
    "        if a != b:\n",
    "            missclassifications += 1\n",
    "            correct_predictions -= 1\n",
    "    return (correct_predictions / len(labels), missclassifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fit_test(c1, c2, c3, testtrain_ratio, dataframe=music_data, verbose=False, learning_rate=0.1, epochs=50, threshold=0):\n",
    "    \"\"\"split data from feature columns c1 and c2 into train and test sets at tt_ratio proportions and fit/test a perceptron\"\"\"\n",
    "    \n",
    "    # get the integer indeces corresponding to the column names passed to split_fit_test\n",
    "    c1_idx = dataframe.columns.get_loc(c1)\n",
    "    c2_idx = dataframe.columns.get_loc(c2)\n",
    "    c3_idx = dataframe.columns.get_loc(c2)\n",
    "    \n",
    "    # number of rows of dataframe which will belong to the training set (we know the number in the test set from this implicitly)\n",
    "    num_train = len(dataframe.index) - int(len(dataframe.index) * testtrain_ratio)\n",
    "    \n",
    "    # Training set\n",
    "    y_train = dataframe.iloc[:num_train,9].values # the array of target values: 2 for benign, 4 for malignant\n",
    "    y_train = np.where(y_train == 1, 1, -1) # change class labels 2 and 4 to -1 and 1 respectively\n",
    "    X_train = dataframe.iloc[:num_train, [c1_idx,c2_idx,c3_idx]].values\n",
    "    \n",
    "    # feature scaling to standardize the distribution of values in our training set\n",
    "    X_train_std = np.copy(X_train)\n",
    "    X_train_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
    "    X_train_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
    "    X_train_std[:, 2] = (X_train[:, 2] - X_train[:, 2].mean()) / X_train[:, 2].std()\n",
    "    \n",
    "    # Testing set\n",
    "    y_test = dataframe.iloc[num_train:,9].values # analagous to above\n",
    "    y_test = np.where(y_test == 1, 1, -1)\n",
    "    X_test = dataframe.iloc[num_train:, [c1_idx, c2_idx, c3_idx]].values\n",
    "    \n",
    "    # feature scaling for test set\n",
    "    X_test_std = np.copy(X_train)\n",
    "    X_test_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
    "    X_test_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
    "    X_test_std[:, 2] = (X_train[:, 2] - X_train[:, 2].mean()) / X_train[:, 2].std()\n",
    "    \n",
    "    # instantiate and train an Adaline object\n",
    "    ada = AdalineSGD(learning_rate=learning_rate, epochs=epochs, threshold=threshold)\n",
    "    ada.fit(X_train_std, y_train)\n",
    "\n",
    "    # predict the classes of the test set and calculate accuracy\n",
    "    prediction = ada.predict(X_test_std)\n",
    "    accuracy,misclasses = accuracy_and_misclasses(prediction, y_test)\n",
    "    if verbose:\n",
    "        print(\"For features\", c1, \",\", c2, \"and\", c3, \", and test/train ratio\", testtrain_ratio, \"the perceptron had\", misclasses, \"missclassifications and had an accuracy of\", accuracy, \"\\n\")\n",
    "        \n",
    "    return (accuracy, misclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run:::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For features danceability , liveness and energy , and test/train ratio 0.3 the perceptron had 335 missclassifications and had an accuracy of 0.4462809917355372 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4462809917355372, 335)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_fit_test('danceability', 'liveness', 'energy', 0.3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 1: Maximize accuracy with respect to test/training set ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.5967741935483871 for test/train proportion 0.4 with 325 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_prop = 0\n",
    " \n",
    "for prop in [0.25, 0.3, 0.35, 0.40, 0.45]: # Try out a variety of test/train proportions\n",
    "    acc,miss = split_fit_test('danceability', 'liveness', 'energy', prop)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_prop = prop\n",
    "        \n",
    "        \n",
    "print(\"The highest accuracy was\", best_accuracy, \"for test/train proportion\", best_prop, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 2: Maximize accuracy by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.6247933884297521 for learning rate 0.0001 with 227 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_rate = 0\n",
    "\n",
    "for rate in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]: # Try out a variety of learning rates\n",
    "    acc,miss = split_fit_test('danceability', 'liveness', 'energy', 0.3, learning_rate=rate)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_rate = rate\n",
    "\n",
    "print(\"The highest accuracy was\", best_accuracy, \"for learning rate\", best_rate, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 3: Maximize accuracy with respect to number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.6330578512396694 for 50 epochs with 222 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_num_epochs = 0\n",
    "\n",
    "for n in [10, 20, 30, 40, 50, 75, 100, 200]: # Try out a variety of epochs\n",
    "    acc,miss = split_fit_test('danceability', 'energy', 'liveness', 0.3, learning_rate=0.1, epochs=n)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_num_epochs = n\n",
    "\n",
    "print(\"The highest accuracy was\", best_accuracy, \"for\", best_num_epochs, \"epochs with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 4: Maximize accuracy with respect to threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.625531914893617 for the threshold -1 with 264 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_threshold = 0\n",
    "for theta in [0, 0.1, 0.01, 0.2, 0.5, 1, 2, -1, -2, 3, 4, 6]: # Try out a variety of tolerance threshold values\n",
    "    acc,miss = split_fit_test('danceability', 'energy', 'liveness', 0.35, learning_rate=0.1, epochs=10, threshold=theta)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_threshold = theta\n",
    "        \n",
    "print(\"The highest accuracy was\", best_accuracy, \"for the threshold\", best_threshold, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy we obtained was ~60% (it varies because of the initial shuffling of the music_data dataframe)\n",
    "This model was not as accurate as our models applied to the cancer data set, but some of the features in this data set are based on subjective concepts such as danceability. Beyond that, there is may also simply not be a significant correlation between the features selected and the mode of the song at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
